% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/hpcaClass.R
\name{mhpca}
\alias{mhpca}
\alias{Mhpca}
\title{A Class for `MHPCA` objects}
\usage{
Mhpca(
  hd_obj,
  method = "power",
  ncomp = 3,
  smooth_tuning = NULL,
  sparse_tuning_u = NULL,
  sparse_tuning_nfd = NULL,
  sparse_tuning_fd = NULL,
  centerfns = TRUE,
  alpha_orth = FALSE,
  smoothing_type = "basispen",
  sparse_type_u = "soft",
  sparse_type_nfd = "soft",
  sparse_type_fd = "soft",
  nfold_u = 30,
  nfold_nfd = 30,
  nfold_fd = 30,
  parallel = FALSE,
  sparse_iter = 1,
  tol = 1e-04,
  max_iter = 1000,
  sparse_CV = TRUE,
  cv.pick = "1se",
  smooth_GCV = TRUE,
  pen_nfd = FALSE,
  pen_fd = FALSE,
  pen_u = FALSE
)
}
\arguments{
\item{hd_obj}{An `hd` object representing the multivariate functional data.}

\item{method}{A character string specifying the approach to be used for MFPCA computation.
Options are "power" (the default), which uses the power algorithm, or "eigen",
which uses the eigen decomposition approach.}

\item{ncomp}{The number of functional principal components to retain.}

\item{smooth_tuning}{A list or vector specifying the smoothing regularization parameter(s) for each variable.
If NULL, non-smoothing MFPCA is estimated.}

\item{sparse_tuning_u}{A list or vector specifying the sparsity regularization parameter(s) for each variable.
If NULL, non-sparse MHPCA is estimated.}

\item{sparse_tuning_nfd}{A list or vector specifying the sparsity regularization parameter(s) for each non functional variable.
If NULL, non-sparse MHPCA is estimated.}

\item{sparse_tuning_fd}{A list or vector specifying the sparsity regularization parameter(s) for each functional variable.
If NULL, non-sparse MHPCA is estimated.}

\item{centerfns}{Logical indicating whether to center the functional data before analysis. Default is TRUE.}

\item{alpha_orth}{Logical indicating whether to perform orthogonalization of the regularization parameters.
If `method` is "power", setting `alpha_orth = FALSE` (default) uses the sequential power approach,
while setting `alpha_orth = TRUE` uses the joint power approach.}

\item{smoothing_type}{The type of smoothing penalty to be applied on the coefficients. The types "coefpen" and "basispen" is supported. Default is "coefpen".}

\item{sparse_type_u}{The type of sparse penalty to be applied on the coefficients. The types "soft", "hard" and "SCAD" is supported. Default is "soft".}

\item{sparse_type_nfd}{The type of sparse penalty to be applied on the nfd right singular vectors. The types "soft", "hard" and "SCAD" is supported. Default is "soft".}

\item{sparse_type_fd}{The type of sparse penalty to be applied on the fd right singular vectors. The types "soft", "hard" and "SCAD" is supported. Default is "soft".}

\item{nfold_u}{An integer specifying the number of folds in the sparse cross-validation process for u. Default is 30.}

\item{nfold_nfd}{An integer specifying the number of folds in the sparse cross-validation process for nfd. Default is 30.}

\item{nfold_fd}{An integer specifying the number of folds in the sparse cross-validation process for fd. Default is 30.}

\item{parallel}{`TRUE`/`FALSE` parallel computing of Cross Validation.}

\item{sparse_iter}{number of iteration for sparse parameter selection}

\item{tol}{power algorithm tolerance}

\item{max_iter}{power algorithm max iteration}

\item{sparse_CV}{Logical indicating whether cross-validation should be applied to select the optimal sparse tuning parameter in sequential power approach.
If `sparse_CV = TRUE`, a series of tuning parameters should be provided as a vector with positive number with max equals to number of subjects.
If `sparse_CV = FALSE`, specific tuning parameters are given directly to each principal components. Tuning parameters should be provided as a vector with length equal to `ncomp`.
If the dimensions of input tuning parameters are incorrect, it will be converted to a list internally, and a warning will be issued.}

\item{cv.pick}{CV selection either based on "1se" or "min"}

\item{smooth_GCV}{Logical indicating whether generalized cross-validation should be applied to select the optimal smooth tuning parameter.
If `smooth_GCV = TRUE`, a series of tuning parameters should be provided as a list with length equal to the number of variables.
If a list with incorrect dimensions is provided, it will be converted to a correct list internally, and a warning will be issued.
If `smooth_GCV = FALSE`, specific tuning parameters are given directly. If `method` is "power" and `alpha_orth = FALSE` (sequential power),
tuning parameters should be provided as a list with length equal to the number of variables, where each element is a vector of length `ncomp`.
If `method` is "power" and `alpha_orth = TRUE` (joint power), tuning parameters should be provided as a vector with length equal to the number of variables.
If the dimensions of input tuning parameters are incorrect, it will be converted to a list internally, and a warning will be issued.}

\item{pen_nfd}{Logical indicating whether sparsity penalty in sequential power approach should be applied on nfd right singular vector.}

\item{pen_fd}{Logical indicating whether sparsity penalty in sequential power approach should be applied on fd right singular vector.}

\item{pen_u}{Logical indicating whether penalize non functional object or left singular vector or not.}
}
\description{
The `mhpca` class represents hybrid principal components components.

The `mhpca` class represents regularized functional principal components ('MHFPCs') components.
}
\examples{
require(fda)
# Brownian Bridge simulation on [0,1]
M <- 110 # number of components
N <- 20 # number of instances
n <- 100 # number of grides
t0 <- seq(0, 1, len = n)
j <- 1:M
alpha1 <- list(a1 = 2^seq(0, 1, length.out = 3), a2 = 2^seq(0, 1, length.out = 3))
psi_1 <- function(t, m) sin(m * pi * t) # eigenfunction of BB
psi_2 <- function(t, m) sin((2 * m - 1) * pi / 2 * t) # eigenfunction of BM
PC_1 <- outer(t0, j, FUN = psi_1) # n by M matrix
PC_2 <- outer(t0, j, FUN = psi_2) # n by M matrix
Z <- matrix(rnorm(N * M), nr = M)
lambda <- matrix(2 / (pi * (2 * j - 1)), nr = M, nc = N)
X_1t <- PC_1 \%*\% (lambda * Z)
X_2t <- PC_2 \%*\% (lambda * Z)
noise <- rnorm(n * N, 0, 0.1)
X_1 <- X_1t + noise
X_2 <- X_2t + noise
bs <- create.bspline.basis(c(0, 1), 51)
mdbs <- Basismfd(bs)
mfd1 <- Mfd(X = X_1, mdbs = mdbs)
mfd2 <- Mfd(X = X_2, mdbs = mdbs)
hd_obj <- hd(mfd1, mfd2)
k <- 2
Re0 <- Mhpca(hd_obj, ncomp = k, alpha = c(0, 0))
fpc0 <- Re0$pc_mfd
scores0 <- inprod_hd(hd_obj, fpc0)
dim(scores0)
Re0$alpha
Re1 <- Mhpca(hd_obj, ncomp = k, alpha = alpha1)
Re1$alpha
Re3 <- Mhpca(mfd1, ncomp = k, alpha = alpha1$a1)
Re3$alpha
}
\seealso{
\code{\link{hd}}
}
\section{Public fields}{
\if{html}{\out{<div class="r6-fields">}}
\describe{
\item{\code{pc_mfd}}{An object of class `mvmfd` where the first indices (fields)
represents harmonics and  second indices represents variables}

\item{\code{pc_nfd}}{An object of class `mvnfd` where the first indices (fields)
represents harmonics and  second indices represents variables}

\item{\code{mean_mfd}}{A multivariate functional data object giving the mean function}

\item{\code{mean_nfd}}{A data object giving the mean of non functional objects}
}
\if{html}{\out{</div>}}
}
\section{Active bindings}{
\if{html}{\out{<div class="r6-active-bindings">}}
\describe{
\item{\code{pc_mfd}}{An object of class `mvmfd` where the first indices (fields)
represents harmonics and  second indices represents variables}

\item{\code{pc_nfd}}{An object of class `mvnfd` where the first indices (fields)
represents harmonics and  second indices represents variables}

\item{\code{lsv}}{= Left singular values vectors}

\item{\code{values}}{= The set of eigenvalues}

\item{\code{smooth_tuning}}{= The list of smoothing penalties parameters}

\item{\code{sparse_tuning_u}}{= The list of sparse penalties parameters}

\item{\code{GCVs}}{= Generalized cross validations scores of smoothing penalties parameters.
If both smoothing and sparse tuning penalties are used in the MHPCA method,
this represents the conditional generalized cross-validation scores, which
means it is computed based on the optimal sparse tuning parameter selected via cross validation.}

\item{\code{CVs_u}}{= Cross validations scores of sparse penalties on u parameters}

\item{\code{CVs_nfd}}{= Cross validations scores of sparse penalties on nfd parameters}

\item{\code{mean_mfd}}{A multivariate functional data object giving the mean function}

\item{\code{mean_nfd}}{A data object giving the mean of non functional objects}
}
\if{html}{\out{</div>}}
}
\section{Methods}{
\subsection{Public methods}{
\itemize{
\item \href{#method-mhpca-new}{\code{mhpca$new()}}
\item \href{#method-mhpca-clone}{\code{mhpca$clone()}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-mhpca-new"></a>}}
\if{latex}{\out{\hypertarget{method-mhpca-new}{}}}
\subsection{Method \code{new()}}{
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{mhpca$new(
  hd_obj,
  method = "power",
  ncomp = 3,
  smooth_tuning = NULL,
  sparse_tuning_u = NULL,
  sparse_tuning_nfd = NULL,
  sparse_tuning_fd = NULL,
  centerfns = TRUE,
  alpha_orth = FALSE,
  smoothing_type = "coefpen",
  sparse_type_u = "soft",
  sparse_type_nfd = "soft",
  sparse_type_fd = "soft",
  nfold_u = 30,
  nfold_nfd = 30,
  nfold_fd = 30,
  parallel = FALSE,
  sparse_iter = 1,
  tol = 1e-04,
  max_iter = 1000,
  sparse_CV,
  cv.pick = "1se",
  smooth_GCV,
  pen_nfd = FALSE,
  pen_fd = FALSE,
  pen_u = FALSE
)}\if{html}{\out{</div>}}
}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-mhpca-clone"></a>}}
\if{latex}{\out{\hypertarget{method-mhpca-clone}{}}}
\subsection{Method \code{clone()}}{
The objects of this class are cloneable with this method.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{mhpca$clone(deep = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{deep}}{Whether to make a deep clone.}
}
\if{html}{\out{</div>}}
}
}
}
